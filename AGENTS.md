<!-- Generated by Ruler -->


<!-- Source: .ruler/AGENTS.md -->

# don't fetch or derive app state in useEffect

# core rules

1. Fetch on navigation in route loaders (SSR + streaming); optionally seed via `queryClient.ensureQueryData`. \[1]
2. Do server work on the server via TanStack Start server functions; after mutations call `router.invalidate()` and/or `queryClient.invalidateQueries()`. \[2]
3. Keep page/UI state in the URL with typed search params (`validateSearch`, `Route.useSearch`, `navigate`). \[3]
4. Reserve effects for real external effects only (DOM, subscriptions, analytics). Compute derived state during render; `useMemo` only if expensive. \[4]\[6]
5. Hydration + Suspense: any update that suspends during hydration replaces SSR content with fallbacks. Wrap sync updates that might suspend in `startTransition` (direct import). Avoid rendering `isPending` during hydration. `useSyncExternalStore` always triggers fallbacks during hydration. \[10]
6. Data placement:

   * Server-synced domain data → TanStack DB collections (often powered by TanStack Query via `queryCollectionOptions`, or a sync engine). Read with live queries. \[11]\[12]\[14]
   * Ephemeral UI/session (theme, modals, steppers, optimistic buffers) → zustand or local-only/localStorage collection. Do not mirror server data into zustand. \[16]\[14]
   * Derived views → compute in render or via live queries. \[12]

# if your useEffect did X → use Y

* Fetch on mount/param change → route loader (+ `ensureQueryData`). \[1]
* Submit/mutate → server function → then `router.invalidate()`/`qc.invalidateQueries()`. \[2]
* Sync UI ↔ querystring → typed search params + `navigate`. \[3]
* Derived state → compute during render (`useMemo` only if expensive). \[4]
* Subscribe external stores → `useSyncExternalStore` (expect hydration fallbacks). \[5]\[10]
* DOM/listeners/widgets → small `useEffect`/`useLayoutEffect`. \[6]
* Synced list + optimistic UI → DB query collection + `onInsert`/`onUpdate`/`onDelete` or server fn + invalidate. \[11]\[13]
* Realtime websocket/SSE patches → TanStack DB direct writes (`writeInsert/update/delete/upsert/batch`). \[13]
* Joins/aggregations → live queries. \[12]
* Local-only prefs/cross-tab → localStorage collection (no effects). \[14]

# idioms (names only)

* Loader: `queryClient.ensureQueryData(queryOptions({ queryKey, queryFn }))` → read via `useSuspenseQuery` hydrated from loader. \[1]
* DB query collection: `createCollection(queryCollectionOptions({ queryKey, queryFn, queryClient, getKey }))` → read via live query. \[11]\[12]
* Mutation (server-first): `createServerFn(...).handler(...)` → on success `qc.invalidateQueries`, `router.invalidate`; supports `<form action={serverFn.url}>`. \[2]
* DB persistence handlers: `onInsert`/`onUpdate`/`onDelete` → return `{ refetch?: boolean }`; pair with direct writes when skipping refetch. \[13]
* Search params as state: `validateSearch → Route.useSearch → navigate({ search })`. \[3]
* External store read: `useSyncExternalStore(subscribe, getSnapshot)`. \[5]
* Hydration-safe: `import { startTransition } from 'react'` for sync updates; avoid `useTransition`/`isPending` during hydration. \[10]

# decision checklist

* Needed at render → loader (defer/stream). \[1]\[7]
* User changed data → server fn → invalidate; or DB handlers/direct writes. \[2]\[13]
* Belongs in URL → typed search params. \[3]
* Purely derived → render/live query. \[4]\[12]
* External system only → effect. \[6]
* Hydration sensitive → `startTransition` for sync updates; expect fallbacks from external stores; avoid `isPending` during hydration. \[10]
* SSR/SEO → loader-based fetching with streaming/deferred; dehydrate/hydrate caches and DB snapshots. \[7]

# React 19 helpers

* `useActionState` for form pending/error/result. \[8]
* `use` to suspend on promises. \[9]

# hydration + suspense playbook \[10]

* Rule: sync updates that suspend during hydration → fallback replaces SSR.
* Quick fix: wrap updates with `startTransition` (direct import); re-wrap after `await`.
* Avoid during hydration: using `useTransition` for the update, rendering `isPending`, `useDeferredValue` unless the suspensey child is memoized, any `useSyncExternalStore` mutation.
* Safe during hydration: setting same value with `useState`/`useReducer`, `startTransition`-wrapped sync updates, `useDeferredValue` with `React.memo` around the suspensey child.
* Compiler auto-memoization may help; treat as optimization.

# TanStack DB: when/how \[11]\[12]\[13]\[14]\[15]\[16]

* Use DB for server-synced domain data.
* Load: `queryCollectionOptions` (simple fetch; optional refetch) or sync collections (Electric/Trailbase/RxDB).
* Read: live queries (reactive, incremental; joins, `groupBy`, `distinct`, `order`, `limit`). \[12]
* Writes:

  * Server-first → server fn → `router.invalidate()`/`qc.invalidateQueries()`. \[2]
  * Client-first → `onInsert`/`onUpdate`/`onDelete` (return `{ refetch: false }` if reconciling via direct writes/realtime). \[13]
  * Direct writes → `writeInsert/update/delete/upsert/batch` for websocket/SSE deltas, incremental pagination, server-computed fields; bypass optimistic layer and skip refetch. \[13]
* Behaviors: query collection treats `queryFn` result as full state; empty array deletes all; merge partial fetches before returning. \[13]
* Transaction merging reduces churn:

  * insert+update → merged insert
  * insert+delete → cancel
  * update+delete → delete
  * update+update → single union
  * same type back-to-back → keep latest \[15]
* SSR: per-request store instances; never touch storage during SSR. \[16]\[14]

# SSR/streaming/hydration with router + DB

* In loaders: seed query via `ensureQueryData`; for DB, preload or dehydrate/hydrate snapshots so lists render instantly and stream updates. \[1]\[7]\[12]\[14]
* After mutations: loader-owned → invalidate router/query; DB-owned → let collection refetch or apply direct writes. \[2]\[13]

# micro-recipes

* Avoid first-click spinner after SSR: wrap clicks with `startTransition`; don't render `isPending` until post-hydration. \[10]
* External store during hydration: defer interaction or isolate the suspense boundary; expect fallbacks. \[5]\[10]
* Paginated load-more: fetch next page, then `collection.utils.writeBatch(() => writeInsert(...))` to append without refetching old pages. \[13]
* Realtime patches: `writeUpsert`/`writeDelete` from socket callback inside `writeBatch`. \[13]

# TanStack Start best practices

## Selective SSR

* Default `ssr: true` (change via `getRouter({ defaultSsr: false })`). SPA mode disables all server loaders/SSR.
* Per-route `ssr`: `true` | `'data-only'` | `false`.
* Functional `ssr(props)`: runs only on server initial request; can return `true` | `'data-only'` | `false` based on validated params/search.
* Inheritance: child can only get less SSR (true → `'data-only'` or false; `'data-only'` → false).
* Fallback: first route with `ssr: false` or `'data-only'` renders `pendingComponent` (or `defaultPendingComponent`) at least `minPendingMs` (or `defaultPendingMinMs`).
* Root: you can disable SSR of root route component; `shellComponent` is always SSRed.

## Zustand in TanStack Start

* Use for client/UI/session and push-based domain state (theme, modals, wizards, optimistic UI, websocket buffers). Keep server data in loaders/Query.
* Per-request store instance to avoid SSR leaks; inject via Router context; dehydrate/hydrate via `router.dehydrate`/`router.hydrate` so snapshots stream with the page.
* After navigation resolution, clear transient UI with `router.subscribe('onResolved', ...)`.
* Mutations: do work in server fn → optionally update store optimistically → `router.invalidate` to reconcile with loader data.
* Persist middleware only for client/session; avoid touching storage during SSR.
* Use atomic selectors (`useStore(s => slice)`) and equality helpers.

## Project constraints

* Use pnpm.
* All route files are TypeScript React (`.tsx`).
* Use alias imports: `~` resolves to root `./src`.
* Never update `.env`; update `.env.example` instead.
* Never start the dev server with `pnpm run dev` or `npm run dev`.
* Never create a local pnpm --store

## docs map

\[1] router data loading · \[2] server functions · \[3] search params · \[4] you might not need an effect · \[5] `useSyncExternalStore` · \[6] synchronizing with effects · \[7] SSR/streaming · \[8] `useActionState` · \[9] `use` · \[10] hydration + suspense guide · \[11] TanStack DB query collection · \[12] live queries · \[13] direct writes + persistence handlers · \[14] collections catalog · \[15] transactions + optimistic actions · \[16] zustand in TanStack Start



<!-- Source: .ruler/ai-agents.md -->

<!-- Generated by Ruler -->

<!-- Source: .ruler/ai-agents.md -->

# build agents as chat engines + tools, not raw completions

# core rules

1. Model all AI behavior as chat sessions via `chat`/`ChatEngine` (or `useChat` in React) instead of calling provider SDKs or `fetch` directly. Treat providers as pluggable adapters behind TanStack AI. [1][2]
2. Use structured message types end-to-end: `UIMessage` at the UI boundary, `ModelMessage` inside the engine, `StreamChunk` over the wire. Do not hand-craft provider-native payloads in app code. [1]
3. Define tools once with `toolDefinition` + Zod schemas. Provide `.server()` and/or `.client()` implementations; register them with `chat({ tools })` and the chat client, instead of embedding side effects in prompts. [2]
4. Keep side effects and business logic in tools and server functions. The model decides *when* to call a tool; the tool decides *what* to do. Avoid “prompt-only” workflows that hide logic in natural language. [2][4]
5. Constrain agents with `agentLoopStrategy` (e.g. `maxIterations(5)`) and explicit tool capabilities. Never run open-ended loops or unconstrained “autonomous” flows. [3]
6. Use server tools for DB, network, secrets, and Start server functions; use client tools for browser-only concerns (local storage, DOM, UI state). Never leak secrets or server context through client tools. [2][4]
7. For TanStack Start, prefer `createServerFnTool` so one implementation serves both AI tools and server functions (`.server` + `.serverFn` from a single definition). [4]
8. Always stream. Use `StreamChunk`s to progressively update the UI (`ThinkingStreamChunk`, `ContentStreamChunk`, `ToolCallStreamChunk`, `ToolResultStreamChunk`, `DoneStreamChunk`) instead of waiting for a full completion. [1]
9. Persist chat/session state in your app store or DB and reconstruct context in each `chat` call. Do not rely on provider “memory” features to hold essential state. [1][5]
10. Treat all model output and tool input as untrusted. Log and monitor tool calls, redact PII, and validate all tool inputs/outputs against schemas before acting. [2][5]

# if your agent did X → use Y

- Issued provider SDK calls (`openai.chat.completions.create`, etc.) directly from components
  → Wrap them in `chat({ model, messages })` using the appropriate provider adapter. [1][2]

- Manually encoded provider-specific message arrays in UI code
  → Convert UI state to `UIMessage[]`, map to `ModelMessage[]` once at the server boundary, then hand that to `chat`. [1]

- Implemented your own tool-calling loop (parse tool JSON, run tool, re-call model)
  → Use `chat` + `agentLoopStrategy` and first-class tools; let `ChatEngine` orchestrate tool calls and follow-ups. [1][3]

- Stuffed business logic, validation, or routing into a giant system prompt
  → Move logic into `toolDefinition` + Zod schemas + server functions; keep prompts descriptive, not procedural. [2][4]

- Needed both “plain API” and “AI tool” access to the same functionality
  → Use `createServerFnTool` in TanStack Start and call `.server` from `chat` and `.serverFn` from components/loaders. [4]

- Put database or secret access in client tools or prompts
  → Move that logic into server tools only; client tools should be UI-level utilities with no privileged access. [2][4]

- Tried to store “conversation memory” only on the provider side
  → Store chat history + domain state in your DB/store and rebuild `messages` per request; use embeddings/vector search only as an optional context source. [1][5]

# idioms (names only)

- Core chat call:
  `chat({ model, messages, tools, agentLoopStrategy, maxTokens, temperature })` [1][3]

- React hook (UI state):
  `const { messages, input, setInput, submit, isLoading, stop, abort } = useChat(chatClientOptions)` [2]

- Message types:
  `UIMessage` ↔ `ModelMessage` ↔ `StreamChunk` conversions at the server/UI boundary [1]

- Tool definition (shared):
  `const tool = toolDefinition({ name, description, inputSchema, outputSchema, needsApproval })` → `tool.server(...)` / `tool.client(...)` [2]

- Start integration (isomorphic tool + serverFn):
  `const searchProducts = createServerFnTool({ name, inputSchema, execute })` → `searchProducts.server` (tool) + `searchProducts.serverFn` (direct call) [4]

- Agent loop:
  `agentLoopStrategy: maxIterations(5)` or custom strategy controlling tool-call iterations [3]

- Transport:
  `createChatClient({ transport, tools, initialMessages })` with SSE/HTTP stream or custom [1]

# decision checklist

- Need a one-off completion without tools
  → `chat({ model, messages })` with no `tools`, streaming enabled. [1]

- Need multi-step reasoning with tool calls
  → `chat({ model, messages, tools, agentLoopStrategy: maxIterations(n) })`. [1][3]

- Need DB, APIs, or integrations
  → Server tools (optionally via `createServerFnTool`) with Zod-validated input/output; no direct provider calls from UI. [2][4]

- Need browser-only behavior (UI state, clipboard, notifications)
  → Client tools registered with the chat client; definitions still live in shared code. [2]

- Need user to approve risky actions (payments, deletes, external calls)
  → `needsApproval: true` on the tool; show approval UI based on `ApprovalRequestedStreamChunk`. [2][5]

- Need to support multiple providers
  → Keep prompts + tools provider-agnostic; swap models via configuration (OpenAI, Anthropic, Gemini, Ollama, etc.). [2]

- Need to debug behavior
  → Log `messages`, tool invocations, and `StreamChunk`s (with redaction) at the engine boundary, not all over the UI. [1][5]

# message + streaming model [1]

- `UIMessage` (UI layer)
  - Shape: `{ id, role, parts: (TextPart | ThinkingPart | ToolCallPart | ToolResultPart)[] }`.
  - Optimized for rendering chat UI, tool call visualizations, and thinking indicators.
  - `ThinkingPart` is UI-only; never send back to the model.

- `ModelMessage` (engine/model layer)
  - Shape: `{ role, content, toolCallId?, toolCalls? }`.
  - Minimal, provider-agnostic representation used by `ChatEngine`.
  - Derived from `UIMessage` when calling `chat`.

- `StreamChunk` (transport layer)
  - Discriminated union for streaming:
    - `ContentStreamChunk`, `ThinkingStreamChunk`, `ToolCallStreamChunk`, `ToolResultStreamChunk`, `ToolInputAvailableStreamChunk`, `ApprovalRequestedStreamChunk`, `DoneStreamChunk`, etc.
  - Server emits chunks; client accumulates them into `UIMessage` parts in real time.

- Rules:
  - Do not create provider-native chunks in UI or business code. Always stream via `StreamChunk`.
  - Treat stream as authoritative state; only consider the run finished after `DoneStreamChunk`.

# tools + execution environment [2]

- Definition
  - `toolDefinition({ name, description, inputSchema, outputSchema, needsApproval? })`.
  - Zod schemas drive runtime validation, static types, and the tool’s “contract” with the model.
  - The model sees the tool name, description, and JSON schema, not your implementation details.

- Implementations
  - Server: `tool.server(async ({ input, context }) => { ... })`
    - Access DB, external APIs, secrets, server functions.
    - Registered directly in `chat({ tools: [tool.server, ...] })`.
  - Client: `tool.client(async ({ input, context }) => { ... })`
    - Access UI state, browser APIs, local storage.
    - Registered with the chat client; server only sees the definition.

- Execution flow
  - Server tools: `ChatEngine` executes them directly upon tool call chunks.
  - Client tools: server emits `ToolInputAvailableStreamChunk`; client runs the tool and posts a `ToolResult` back.
  - Approval: with `needsApproval`, engine emits `ApprovalRequestedStreamChunk`; UI must approve/reject before execution.

- Discipline
  - Use tools for any action the model should be able to *decide* to perform.
  - Use regular code paths (serverFns, route handlers) when only humans decide to perform the action.
  - Keep tool outputs structured (JSON) and small; render them nicely in the UI.

# building agents with `chat` + `agentLoopStrategy` [1][3]

- `chat` is the primary entry:
  - Inputs: `model`, `messages`, optional `tools`, `agentLoopStrategy`, streaming adapter, and configuration.
  - Output: `StreamChunk` stream + final result object.

- Agent loop
  - Default: `maxIterations(5)` – run model → maybe call tools → produce answer, up to N iterations.
  - Custom strategies: implement custom stopping logic (e.g., budget, time, required tools).
  - Do not expose unlimited loops; enforce a max iteration count.

- Prompts
  - System messages define role and objectives, but should reference tools explicitly and succinctly.
  - Avoid encoding full workflows in text; rely on tool affordances and loop strategy.

# React bindings + chat client [2]

- Chat client
  - `createChatClient({ transport, tools, initialMessages })` lives in your app shell.
  - Handles streaming, reconnection, tool invocations, and mapping `StreamChunk` → `UIMessage`.

- React state
  - `useChat(chatClientOptions)` exposes `messages`, form state, submit handlers, and pending status.
  - UI components should only know about `UIMessage` and not about provider specifics.

- Pattern
  - UI → `useChat` → chat client → transport → server `chat` → provider.
  - Only the server side sees the model adapter and environment; UI stays model/provider agnostic.

# TanStack Start integration [4]

- `createServerFnTool`
  - Define once:

    ```ts
    const searchProducts = createServerFnTool({
      name: 'searchProducts',
      inputSchema: z.object({ query: z.string() }),
      execute: async ({ query }, ctx) => db.products.search(query),
    })
    ```

  - As an AI tool: use `searchProducts.server` in `chat({ tools: [...] })`.
  - As a server function: call `await searchProducts.serverFn({ query })` from loaders/components.

- Benefits
  - No duplicate logic between “regular” serverFns and AI tools.
  - Shared types and validation, easy testability.
  - Works with Start’s SSR, streaming, and route loaders.

# micro-recipes

- “Chat with tools” agent
  - Define domain tools with `toolDefinition` / `createServerFnTool`.
  - Configure `chat({ model, messages, tools, agentLoopStrategy: maxIterations(3) })`.
  - Stream chunks to UI; render thinking + tool calls in a side panel.

- Human-in-the-loop approvals
  - Mark sensitive tools with `needsApproval: true`.
  - In the client, watch for `ApprovalRequestedStreamChunk`.
  - Present a diff/summary and explicit Approve/Reject actions; on approve, trigger the tool execution path.

- Hybrid server/client tools
  - Single definition; implement `.server` for secure behavior and `.client` for UI-only enhancements.
  - Register both, letting the engine route execution based on environment.

- Provider swap
  - Encapsulate model choice in configuration/env (`model: openai('gpt-4.1'), anthropic('claude-3.5'), ollama('llama3')`).
  - Keep prompts/tool contracts stable so you can switch models without app changes.

# project constraints

- Use `@tanstack/ai` (core) + `@tanstack/ai-react` for React bindings.
- Keep provider SDK usage confined to model adapter configuration; do not scatter provider code across the app.
- Co-locate tool definitions in `src/ai/tools` (or similar), split into `definitions`, `server`, `client`, and `start` integration modules.
- All tools and models must be fully typed via Zod and TypeScript.
- Never embed secrets, keys, or raw provider URLs in client bundles.

# docs map

- [1] Data flow, message types, and ChatEngine (TanStack AI docs: core concepts, data flow, message model)
- [2] Tool system (toolDefinition, server/client tools, tool execution protocol)
- [3] Agent loop strategies and multi-step agents (agentLoopStrategy, iterations)
- [4] TanStack Start integration (createServerFnTool, sharing tools and serverFns)
- [5] Safety, approvals, and production hardening (needsApproval, validation, logging, PII handling)



<!-- Source: .ruler/general-rules.md -->

### General Rules

— Early development, no users. No backwards compatibility concerns. Do things RIGHT:
organized, zero tech debt. Never create compatibility shims.

- WE NEVER WANT WORKAROUNDS. we always want FULL implementations that are long term
sustainable for many >l000 users. so dont come up with half baked solutions
- Important: Do not remove, hide, or rename any existing features or UI options (even
temporarily) unless I explicitly ask for it. If something isn't fully wired yet, keep the UX surface intact and stub/annotate it instead of deleting it.



<!-- Source: .ruler/missin-docs-use-mcp.md -->

# mcp-doc-context-collection-before-starting-work-always

## Doc Tool Selection and usage

### contex7

Always use context7 when I need code generation, setup or configuration steps, or
library/API documentation. This means you should automatically use the Context7 MCP
tools to resolve library id and get library docs without me having to explicitly ask.

### MCPdoc server

for ANY work relating to <libraries> and/or providers use the <name>-docs-mcp server to help answer --

- call list_doc_sources tool to get the available llms.txt file
- call fetch_docs tool to read it
- reflect on the urls in llms.txt
- reflect on the input question
- call fetch_docs on any urls relevant to the question
- use this to answer the question

---

### gitmcp Server usage

for ANY work or questions relating to <libraries> and/or providers use the <name>_docs server for that repo:

- call search_<name>_documentation to find relevant docs
- call fetch_<name>_documentation to read the primary docs
- reflect on the input question
- call fetch_url_content for any external URLs referenced
- call search_<name>_code if code locations are referenced or needed
- use this to answer the question

---



<!-- Source: .ruler/skill-usage.md -->

# Codex skills usage

- For every UI/frontend related task (new components, refactoring existing component, scanning through existing components, removing/replacing components, etc. ), first interpret the task under the applicable `AGENTS.md` rules, then apply `~/.codex/prompts/codex-skills/frontend-design/SKILL.md`. Follow these instructions until work is completed.
- For every logic/backend task (refactors, features, tests, docs, workflows, integrations, etc.), first interpret the task under the applicable `AGENTS.md` rules, then scan `~/.codex/prompts/codex-skills/` and select the skill whose `SKILL.md` description and metadata most closely match the requested work.
- Once a relevant skill is identified, follow its `SKILL.md` procedure, structure, and output formats, but resolve any conflict in favor of the current directory’s `AGENTS.md`.
- If no suitable skill exists, use the prompts under `~/.codex/prompts/codex-skills/skill-creator/` to define a new skill for the task, save it into the appropriate subdirectory under `codex-skills`, then re-run the task using that newly created skill, still subject to the governing `AGENTS.md` rules.
