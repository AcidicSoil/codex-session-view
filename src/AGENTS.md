<!-- Generated by Ruler -->


<!-- Source: .ruler/ai-agents.md -->

# build agents as chat engines + tools, not raw completions

# core rules

1. Model all AI behavior as chat sessions via `chat`/`ChatEngine` (or `useChat` in React) instead of calling provider SDKs or `fetch` directly. Treat providers as pluggable adapters behind TanStack AI. [1][2]
2. Use structured message types end-to-end: `UIMessage` at the UI boundary, `ModelMessage` inside the engine, `StreamChunk` over the wire. Do not hand-craft provider-native payloads in app code. [1]
3. Define tools once with `toolDefinition` + Zod schemas. Provide `.server()` and/or `.client()` implementations; register them with `chat({ tools })` and the chat client, instead of embedding side effects in prompts. [2]
4. Keep side effects and business logic in tools and server functions. The model decides *when* to call a tool; the tool decides *what* to do. Avoid “prompt-only” workflows that hide logic in natural language. [2][4]
5. Constrain agents with `agentLoopStrategy` (e.g. `maxIterations(5)`) and explicit tool capabilities. Never run open-ended loops or unconstrained “autonomous” flows. [3]
6. Use server tools for DB, network, secrets, and Start server functions; use client tools for browser-only concerns (local storage, DOM, UI state). Never leak secrets or server context through client tools. [2][4]
7. For TanStack Start, prefer `createServerFnTool` so one implementation serves both AI tools and server functions (`.server` + `.serverFn` from a single definition). [4]
8. Always stream. Use `StreamChunk`s to progressively update the UI (`ThinkingStreamChunk`, `ContentStreamChunk`, `ToolCallStreamChunk`, `ToolResultStreamChunk`, `DoneStreamChunk`) instead of waiting for a full completion. [1]
9. Persist chat/session state in your app store or DB and reconstruct context in each `chat` call. Do not rely on provider “memory” features to hold essential state. [1][5]
10. Treat all model output and tool input as untrusted. Log and monitor tool calls, redact PII, and validate all tool inputs/outputs against schemas before acting. [2][5]

# if your agent did X → use Y

- Issued provider SDK calls (`openai.chat.completions.create`, etc.) directly from components
  → Wrap them in `chat({ model, messages })` using the appropriate provider adapter. [1][2]

- Manually encoded provider-specific message arrays in UI code
  → Convert UI state to `UIMessage[]`, map to `ModelMessage[]` once at the server boundary, then hand that to `chat`. [1]

- Implemented your own tool-calling loop (parse tool JSON, run tool, re-call model)
  → Use `chat` + `agentLoopStrategy` and first-class tools; let `ChatEngine` orchestrate tool calls and follow-ups. [1][3]

- Stuffed business logic, validation, or routing into a giant system prompt
  → Move logic into `toolDefinition` + Zod schemas + server functions; keep prompts descriptive, not procedural. [2][4]

- Needed both “plain API” and “AI tool” access to the same functionality
  → Use `createServerFnTool` in TanStack Start and call `.server` from `chat` and `.serverFn` from components/loaders. [4]

- Put database or secret access in client tools or prompts
  → Move that logic into server tools only; client tools should be UI-level utilities with no privileged access. [2][4]

- Tried to store “conversation memory” only on the provider side
  → Store chat history + domain state in your DB/store and rebuild `messages` per request; use embeddings/vector search only as an optional context source. [1][5]

# idioms (names only)

- Core chat call:
  `chat({ model, messages, tools, agentLoopStrategy, maxTokens, temperature })` [1][3]

- React hook (UI state):
  `const { messages, input, setInput, submit, isLoading, stop, abort } = useChat(chatClientOptions)` [2]

- Message types:
  `UIMessage` ↔ `ModelMessage` ↔ `StreamChunk` conversions at the server/UI boundary [1]

- Tool definition (shared):
  `const tool = toolDefinition({ name, description, inputSchema, outputSchema, needsApproval })` → `tool.server(...)` / `tool.client(...)` [2]

- Start integration (isomorphic tool + serverFn):
  `const searchProducts = createServerFnTool({ name, inputSchema, execute })` → `searchProducts.server` (tool) + `searchProducts.serverFn` (direct call) [4]

- Agent loop:
  `agentLoopStrategy: maxIterations(5)` or custom strategy controlling tool-call iterations [3]

- Transport:
  `createChatClient({ transport, tools, initialMessages })` with SSE/HTTP stream or custom [1]

# decision checklist

- Need a one-off completion without tools
  → `chat({ model, messages })` with no `tools`, streaming enabled. [1]

- Need multi-step reasoning with tool calls
  → `chat({ model, messages, tools, agentLoopStrategy: maxIterations(n) })`. [1][3]

- Need DB, APIs, or integrations
  → Server tools (optionally via `createServerFnTool`) with Zod-validated input/output; no direct provider calls from UI. [2][4]

- Need browser-only behavior (UI state, clipboard, notifications)
  → Client tools registered with the chat client; definitions still live in shared code. [2]

- Need user to approve risky actions (payments, deletes, external calls)
  → `needsApproval: true` on the tool; show approval UI based on `ApprovalRequestedStreamChunk`. [2][5]

- Need to support multiple providers
  → Keep prompts + tools provider-agnostic; swap models via configuration (OpenAI, Anthropic, Gemini, Ollama, etc.). [2]

- Need to debug behavior
  → Log `messages`, tool invocations, and `StreamChunk`s (with redaction) at the engine boundary, not all over the UI. [1][5]

# message + streaming model [1]

- `UIMessage` (UI layer)
  - Shape: `{ id, role, parts: (TextPart | ThinkingPart | ToolCallPart | ToolResultPart)[] }`.
  - Optimized for rendering chat UI, tool call visualizations, and thinking indicators.
  - `ThinkingPart` is UI-only; never send back to the model.

- `ModelMessage` (engine/model layer)
  - Shape: `{ role, content, toolCallId?, toolCalls? }`.
  - Minimal, provider-agnostic representation used by `ChatEngine`.
  - Derived from `UIMessage` when calling `chat`.

- `StreamChunk` (transport layer)
  - Discriminated union for streaming:
    - `ContentStreamChunk`, `ThinkingStreamChunk`, `ToolCallStreamChunk`, `ToolResultStreamChunk`, `ToolInputAvailableStreamChunk`, `ApprovalRequestedStreamChunk`, `DoneStreamChunk`, etc.
  - Server emits chunks; client accumulates them into `UIMessage` parts in real time.

- Rules:
  - Do not create provider-native chunks in UI or business code. Always stream via `StreamChunk`.
  - Treat stream as authoritative state; only consider the run finished after `DoneStreamChunk`.

# tools + execution environment [2]

- Definition
  - `toolDefinition({ name, description, inputSchema, outputSchema, needsApproval? })`.
  - Zod schemas drive runtime validation, static types, and the tool’s “contract” with the model.
  - The model sees the tool name, description, and JSON schema, not your implementation details.

- Implementations
  - Server: `tool.server(async ({ input, context }) => { ... })`
    - Access DB, external APIs, secrets, server functions.
    - Registered directly in `chat({ tools: [tool.server, ...] })`.
  - Client: `tool.client(async ({ input, context }) => { ... })`
    - Access UI state, browser APIs, local storage.
    - Registered with the chat client; server only sees the definition.

- Execution flow
  - Server tools: `ChatEngine` executes them directly upon tool call chunks.
  - Client tools: server emits `ToolInputAvailableStreamChunk`; client runs the tool and posts a `ToolResult` back.
  - Approval: with `needsApproval`, engine emits `ApprovalRequestedStreamChunk`; UI must approve/reject before execution.

- Discipline
  - Use tools for any action the model should be able to *decide* to perform.
  - Use regular code paths (serverFns, route handlers) when only humans decide to perform the action.
  - Keep tool outputs structured (JSON) and small; render them nicely in the UI.

# building agents with `chat` + `agentLoopStrategy` [1][3]

- `chat` is the primary entry:
  - Inputs: `model`, `messages`, optional `tools`, `agentLoopStrategy`, streaming adapter, and configuration.
  - Output: `StreamChunk` stream + final result object.

- Agent loop
  - Default: `maxIterations(5)` – run model → maybe call tools → produce answer, up to N iterations.
  - Custom strategies: implement custom stopping logic (e.g., budget, time, required tools).
  - Do not expose unlimited loops; enforce a max iteration count.

- Prompts
  - System messages define role and objectives, but should reference tools explicitly and succinctly.
  - Avoid encoding full workflows in text; rely on tool affordances and loop strategy.

# React bindings + chat client [2]

- Chat client
  - `createChatClient({ transport, tools, initialMessages })` lives in your app shell.
  - Handles streaming, reconnection, tool invocations, and mapping `StreamChunk` → `UIMessage`.

- React state
  - `useChat(chatClientOptions)` exposes `messages`, form state, submit handlers, and pending status.
  - UI components should only know about `UIMessage` and not about provider specifics.

- Pattern
  - UI → `useChat` → chat client → transport → server `chat` → provider.
  - Only the server side sees the model adapter and environment; UI stays model/provider agnostic.

# TanStack Start integration [4]

- `createServerFnTool`
  - Define once:

    ```ts
    const searchProducts = createServerFnTool({
      name: 'searchProducts',
      inputSchema: z.object({ query: z.string() }),
      execute: async ({ query }, ctx) => db.products.search(query),
    })
    ```

  - As an AI tool: use `searchProducts.server` in `chat({ tools: [...] })`.
  - As a server function: call `await searchProducts.serverFn({ query })` from loaders/components.

- Benefits
  - No duplicate logic between “regular” serverFns and AI tools.
  - Shared types and validation, easy testability.
  - Works with Start’s SSR, streaming, and route loaders.

# micro-recipes

- “Chat with tools” agent
  - Define domain tools with `toolDefinition` / `createServerFnTool`.
  - Configure `chat({ model, messages, tools, agentLoopStrategy: maxIterations(3) })`.
  - Stream chunks to UI; render thinking + tool calls in a side panel.

- Human-in-the-loop approvals
  - Mark sensitive tools with `needsApproval: true`.
  - In the client, watch for `ApprovalRequestedStreamChunk`.
  - Present a diff/summary and explicit Approve/Reject actions; on approve, trigger the tool execution path.

- Hybrid server/client tools
  - Single definition; implement `.server` for secure behavior and `.client` for UI-only enhancements.
  - Register both, letting the engine route execution based on environment.

- Provider swap
  - Encapsulate model choice in configuration/env (`model: openai('gpt-4.1'), anthropic('claude-3.5'), ollama('llama3')`).
  - Keep prompts/tool contracts stable so you can switch models without app changes.

# project constraints

- Use `@tanstack/ai` (core) + `@tanstack/ai-react` for React bindings.
- Keep provider SDK usage confined to model adapter configuration; do not scatter provider code across the app.
- Co-locate tool definitions in `src/ai/tools` (or similar), split into `definitions`, `server`, `client`, and `start` integration modules.
- All tools and models must be fully typed via Zod and TypeScript.
- Never embed secrets, keys, or raw provider URLs in client bundles.

# docs map

- [1] Data flow, message types, and ChatEngine (TanStack AI docs: core concepts, data flow, message model)
- [2] Tool system (toolDefinition, server/client tools, tool execution protocol)
- [3] Agent loop strategies and multi-step agents (agentLoopStrategy, iterations)
- [4] TanStack Start integration (createServerFnTool, sharing tools and serverFns)
- [5] Safety, approvals, and production hardening (needsApproval, validation, logging, PII handling)



<!-- Source: .ruler/tanstack-environment-server-client-only-rules.md -->

# ClientOnly

Client-only render to avoid SSR hydration issues. Import from `@tanstack/react-router`:

```typescript
import { ClientOnly } from '@tanstack/react-router';

<ClientOnly fallback={<span>—</span>}>
  <ComponentThatUsesClientHooks />
</ClientOnly>
```

Alternative: Custom implementation using mounted pattern if needed (see hydration errors below).

# Environment functions

From `@tanstack/react-start`:

## createIsomorphicFn

Adapts to client/server:

```typescript
import { createIsomorphicFn } from '@tanstack/react-start';
const getEnv = createIsomorphicFn()
  .server(() => 'server')
  .client(() => 'client');
getEnv(); // 'server' on server, 'client' on client
```

Partial: `.server()` no-op on client, `.client()` no-op on server.

## createServerOnlyFn / createClientOnlyFn

RC1: `serverOnly` → `createServerOnlyFn`, `clientOnly` → `createClientOnlyFn`

Strict environment execution (throws if called wrong env):

```typescript
import { createServerOnlyFn, createClientOnlyFn } from '@tanstack/react-start';
const serverFn = createServerOnlyFn(() => 'bar'); // throws on client
const clientFn = createClientOnlyFn(() => 'bar'); // throws on server
```

Tree-shaken: client code removed from server bundle, server code removed from client bundle.

# Hydration errors

Mismatch: Server HTML differs from client render. Common causes: Intl (locale/timezone), Date.now(), random IDs, responsive logic, feature flags, user prefs.

Strategies:
1. Make server and client match: deterministic locale/timezone on server (cookie or Accept-Language header), compute once and hydrate as initial state.
2. Let client tell environment: set cookie with client timezone on first visit, SSR uses UTC until then.
3. Make it client-only: wrap unstable UI in `<ClientOnly>` to avoid SSR mismatches.
4. Disable/limit SSR: use selective SSR (`ssr: 'data-only'` or `false`) when server HTML cannot be stable.
5. Last resort: React's `suppressHydrationWarning` for small known-different nodes (use sparingly).

Checklist: Deterministic inputs (locale, timezone, feature flags). Prefer cookies for client context. Use `<ClientOnly>` for dynamic UI. Use selective SSR when server HTML unstable. Avoid blind suppression.

# TanStack Start basics

Depends: @tanstack/react-router, Vite. Router: getRouter() (was createRouter() in beta). routeTree.gen.ts auto-generated on first dev run. Optional: server handler via @tanstack/react-start/server; client hydrate via StartClient from @tanstack/react-start/client. RC1: Import StartClient from @tanstack/react-start/client (not @tanstack/react-start). StartClient no longer requires router prop. Root route head: utf-8, viewport, title; component wraps Outlet in RootDocument. Routes: createFileRoute() code-split + lazy-load; loader runs server/client. Navigation: Link (typed), useNavigate (imperative), useRouter (instance).

# Server functions

createServerFn({ method }) + zod .inputValidator + .handler(ctx). After mutations: router.invalidate(); queryClient.invalidateQueries(['entity', id]).

# Typed Links

Link to="/posts/$postId" with params; activeProps for styling.



<!-- Source: .ruler/tanstack-query-rules.md -->

# TanStack Query Rules

Server state via TanStack Query + server functions. Type-safe fetching and mutations.

## Query Pattern

Define in `lib/{resource}/queries.ts` using `queryOptions`:

```typescript
export const todosQueryOptions = () =>
  queryOptions({
    queryKey: ['todos'],
    queryFn: async ({ signal }) => await getTodos({ signal }),
    staleTime: 1000 * 60 * 5,
    gcTime: 1000 * 60 * 10,
  });
```

Use: `const { data, isLoading } = useQuery(todosQueryOptions())`. Prefer `useSuspenseQuery` with Suspense.

## Server Functions in Queries

Call server functions directly in `queryFn`. No `useServerFn` hook. TanStack Start proxies. Pass `signal` for cancellation.

## Mutation Pattern

```typescript
const mutation = useMutation({
  mutationFn: async (text: string) => await createTodo({ data: { text } }),
  onSuccess: () => {
    queryClient.invalidateQueries({ queryKey: todosQueryOptions().queryKey });
    toast.success('Success');
  },
  onError: (error) => toast.error(error.message || 'Failed'),
});
```

Call via `mutation.mutate(data)` or `mutateAsync` for promises.

## Query Invalidation

After mutations: `queryClient.invalidateQueries({ queryKey: ... })`. Use specific keys, not broad.

## Mutation States

Access: `isPending`, `isError`, `isSuccess`, `error`, `data`. Disable UI during `isPending`.

## Error Handling

Handle in `onError`. Toast messages. Access: `error.message || 'Default'`.

## Query Keys

Hierarchical: `['todos']`, `['todo', id]`, `['todos', 'completed']`. Include all affecting variables.

## Stale Time vs GC Time

`staleTime`: freshness duration (no refetch). Default 0. Set for stable data.
`gcTime`: unused cache duration (was `cacheTime`). Default 5min. Memory management.

## Infinite Queries

`useInfiniteQuery` for pagination. Required: `initialPageParam`, `getNextPageParam`, `fetchNextPage`. Access `data.pages`. Check `hasNextPage` before fetching.

## Optimistic Updates

`onMutate` for optimistic updates. Rollback in `onError`. Update cache via `queryClient.setQueryData`.

## Best Practices

1. Queries in `lib/{resource}/queries.ts` with `queryOptions`
2. Call server functions directly (no `useServerFn` in callbacks)
3. Invalidate after mutations
4. Toast for feedback
5. Handle loading/error states
6. Use TypeScript types from query options
7. Set `staleTime`/`gcTime` appropriately
8. Prefer `useSuspenseQuery` with Suspense
