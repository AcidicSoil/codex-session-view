# Required by env schema
MY_SECRET_VAR=https://example.com

# Enable the dual-mode chatbot stack locally
SESSION_COACH_ENABLED=true

# AI runtime configuration
#AI_OPENAI_COMPATIBLE_API_KEY=your-openai-key
#AI_OPENAI_COMPATIBLE_BASE_URL=https://api.openai.com/v1
#AI_SESSION_DEFAULT_MODEL=openai:gpt-4o-mini
#AI_GENERAL_DEFAULT_MODEL=openai:gpt-4o-mini
# Uncomment the following lines to use the deterministic offline models in CI/dev
# AI_SESSION_DEFAULT_MODEL=demo:grounded
# AI_GENERAL_DEFAULT_MODEL=demo:general
#GEMINI_API_KEY=your-gemini-key-if-needed
GEMINI_AUTH_TYPE=oauth-personal
GEMINI_CACHE_DIR=
GEMINI_PROXY=
#GEMINI_SESSION_DIR=
# Database + ElectricSQL
# Required for Postgres persistence + Electric sync engine
DATABASE_URL=postgres://user:password@localhost:5432/codex_session_view
ELECTRIC_HTTP_URL=http://localhost:3000/v1/shape
ELECTRIC_SYNC_URL=ws://localhost:3010

# Codex CLI defaults (auth handled by `codex login`)
AI_CODEX_CLI_ALLOW_NPX=true
AI_CODEX_CLI_SKIP_GIT_REPO_CHECK=true
AI_CODEX_CLI_APPROVAL_MODE=on-failure
AI_CODEX_CLI_SANDBOX_MODE=workspace-write
AI_CODEX_CLI_VERBOSE=false
AI_CODEX_CLI_API_KEY=
AI_CODEX_CLI_PATH=

# LM Studio OpenAI-compatible server
AI_LMSTUDIO_BASE_URL=http://127.0.0.1:1234/v1
AI_LMSTUDIO_API_KEY=lm-studio # LM Studio ignores the value but requires a string. Run `lms server start` before selecting LM Studio in the UI.

# Deployment helpers
# When simulating Vercel builds locally, set DEPLOY_TARGET=vercel before running `pnpm run build`.
