This report synthesizes implementation strategies for persisting LLM tool-call activity, specifically tailored for **TypeScript**, **TanStack Start**, and the **Vercel AI SDK**.

The core finding is that modern "Agentic" applications require a fundamental shift in data modeling: moving from a simple "list of strings" to a **"Multi-Part Event Stream"**. The architecture below decouples **Context Persistence** (what the LLM needs to know next turn) from **Audit Logging** (what developers/compliance need to see).

### 1\. The Core Architecture: "Message Parts" vs. "Blobs"

The most critical decision is your database schema. In early LLM apps, developers stored chat history as a single JSON blob (`jsonb`). **Do not do this for tool-use agents.**

When an LLM calls a tool, it generates multiple distinct events:

1. **Reasoning:** "I need to check the stock price."
2. **Invocation:** `call_tool('get_stock_price', { symbol: 'AAPL' })`
3. **Result:** `{'price': 150.00}` (Generated by your code, not the LLM)

If you store this as a single blob, you lose the ability to query tool usage (e.g., "How often does `get_stock_price` fail?") and makes "rehydrating" the UI complex.

#### Recommended Prisma Schema (Postgres)

This normalized schema treats `ToolInvocation` as a first-class citizen, linked to a specific part of a message.

```prisma
// schema.prisma

model Conversation {
  id        String    @id @default(cuid())
  messages  Message
}

model Message {
  id             String        @id @default(cuid())
  role           String        // 'user', 'assistant', 'system'
  conversationId String
  conversation   Conversation  @relation(fields: [conversationId], references: [id])

  // A single message can contain text AND tool calls (e.g., "Here is the data:" + [Chart])
  parts          MessagePart

  createdAt      DateTime      @default(now())
}

model MessagePart {
  id              String          @id @default(cuid())
  messageId       String
  type            String          // 'text', 'reasoning', 'tool-invocation'
  position        Int             // Critical for preserving order

  // Content fields (nullable depending on type)
  text            String?         @db.Text

  // Relations
  toolInvocation  ToolInvocation?
  message         Message         @relation(fields: [messageId], references: [id], onDelete: Cascade)
}

model ToolInvocation {
  id             String      @id @default(cuid())
  partId         String      @unique
  part           MessagePart @relation(fields: [partId], references: [id], onDelete: Cascade)

  // Link to the LLM's internal ID (Vital for context continuity)
  toolCallId     String
  toolName       String

  // Use JSONB for flexibility as tool arguments change over time
  args           Json
  result         Json?

  // State tracking: 'call', 'result' (success), 'error'
  state          String
}
```

-----

### 2\. Server-Side: TanStack Start Implementation

In TanStack Start, you use `createServerFn` to expose an RPC endpoint. The challenge is persisting data without blocking the streaming response to the user.

**Strategy:** Use the `onFinish` callback in Vercel AI SDK to perform a "Double Write"â€”streaming to the user immediately while asynchronously writing to Postgres.

```typescript
// app/functions/chat.ts
import { createServerFn } from '@tanstack/start';
import { streamText, convertToCoreMessages } from 'ai';
import { openai } from '@ai-sdk/openai';
import { prisma } from '@/lib/prisma';

export const sendMessage = createServerFn({ method: 'POST' })
 .validator((data: { messages: any; conversationId: string }) => data)
 .handler(async ({ data }) => {
    const { messages, conversationId } = data;

    // 1. Persist User Message immediately (before LLM generation)
    const userMsgText = messages[messages.length - 1].content;
    await prisma.message.create({
      data: {
        conversationId,
        role: 'user',
        parts: { create: }
      }
    });

    // 2. Start Streaming
    const result = streamText({
      model: openai('gpt-4o'),
      messages: convertToCoreMessages(messages),
      tools: {
        //... your tool definitions
      },
      // 3. PERSISTENCE INTERCEPTOR
      onFinish: async ({ response }) => {
        // This runs after the stream closes. We save the assistant's FULL response.
        const assistantMsg = await prisma.message.create({
          data: { conversationId, role: 'assistant' }
        });

        const partsToSave = response.messages.flatMap((msg, msgIndex) => {
          if (msg.role!== 'assistant') return;

          const parts =;

          // Save Text Content
          if (msg.content) {
            parts.push({
              messageId: assistantMsg.id,
              type: 'text',
              position: msgIndex,
              text: typeof msg.content === 'string'? msg.content : ''
            });
          }

          // Save Tool Calls
          if (msg.toolCalls) {
            msg.toolCalls.forEach((tc, tcIndex) => {
              parts.push({
                messageId: assistantMsg.id,
                type: 'tool-invocation',
                position: msgIndex + tcIndex + 1,
                toolInvocation: {
                  create: {
                    toolCallId: tc.toolCallId,
                    toolName: tc.toolName,
                    args: tc.args,
                    state: 'result', // or 'call' if it failed/requires confirmation
                    result: 'result' in tc? tc.result : undefined,
                  }
                }
              });
            });
          }
          return parts;
        });

        // Batch insert parts (Prisma transaction)
        for (const part of partsToSave) {
           // Prisma create logic here...
        }
      },
    });

    // 4. Return the stream response (TanStack Start handles the Response object)
    return result.toTextStreamResponse();
  });
```

-----

### 3\. Client-Side: Rehydration (The "Flicker" Problem)

When a user reloads the page, you must fetch the history from Prisma and map it back to the Vercel AI SDK's expected format. If you fail to map the `toolInvocations` correctly, the UI will show raw JSON instead of your nice React components.

**The Loader Pattern:**

```typescript
// app/routes/chat/$chatId.tsx
import { createFileRoute } from '@tanstack/react-router';
import { useChat } from '@ai-sdk/react';

export const Route = createFileRoute('/chat/$chatId')({
  loader: async ({ params }) => {
    const dbMessages = await getMessagesFromDb(params.chatId);

    // TRANSFORMER: DB -> UI Format
    return dbMessages.map(m => ({
      id: m.id,
      role: m.role,
      content: m.parts.filter(p => p.type === 'text').map(p => p.text).join(''),
      // CRITICAL: Reconstruct tool invocations
      toolInvocations: m.parts
       .filter(p => p.type === 'tool-invocation')
       .map(p => ({
          state: p.toolInvocation.state,
          toolCallId: p.toolInvocation.toolCallId,
          toolName: p.toolInvocation.toolName,
          args: p.toolInvocation.args,
          result: p.toolInvocation.result,
        })),
    }));
  },
  component: ChatComponent
});

function ChatComponent() {
  const initialMessages = Route.useLoaderData();

  // Pass transformed messages to useChat
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    initialMessages,
    //...
  });

  // Render loop...
}
```

### 4\. Audit Trails vs. Context History

A common pitfall is treating the chat history as the *only* log.

* **Chat History (Prisma):** Mutable. Optimized for "What happened in this thread?"
* **Audit Trail (Observability):** Immutable. Optimized for "Why did the agent fail?" or "Did we leak PII?"

**Recommendation:** Do not clutter your Postgres DB with raw HTTP traces or token usage stats.
Use **OpenLLMetry** (based on OpenTelemetry) to pipe these traces to a dedicated backend like **LangFuse** or **Arize Phoenix**.

* **Implementation:** Wrap your `streamText` call with OpenLLMetry's tracer. It automatically captures latency, token count, and raw tool inputs/outputs without requiring schema changes in Postgres.

### Summary Checklist for Implementation

1. **Schema:** Create `MessagePart` and `ToolInvocation` tables; do not rely on JSON blobs for tool data.
2. **Server:** Use `onFinish` in `streamText` to persist data asynchronously.
3. **UI:** Create a transformation utility in your Loader to map DB records to `initialMessages`.
4. **Components:** Ensure your `ToolInvocation` UI components handle both `state: 'call'` (loading) and `state: 'result'` (finished) to prevent UI jumping during history load.
